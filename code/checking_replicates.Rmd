---
title: "deeper look at pcr replicate variability"
author: "Kimberly Ledger"
date: "2024-07-02"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

libraries
```{r}
library(tidyverse)
rename <- dplyr::rename
```

load sample type and other library prep info
```{r}
sample_metadata <- read.csv("/home/kimberly.ledger/BeringSea_Arctic_eDNA/data/NBS_SBS_DBO_sample_names.csv")

#illumina output changed "_" to "-"
sample_metadata$sample_ID <- gsub("_", "-", sample_metadata$sample_ID) 
sample_metadata$sample_ID_date <- gsub("_", "-", sample_metadata$sample_ID_date) 
```

check sequence table outputs
```{r}
asv_table <- readRDS("/home/kimberly.ledger/BeringSea_Arctic_eDNA/data/NBS_SBS_DBO_20240703/filtered.seqTab.RDS") %>%
  select(!Row.names)

#transpose 
asv_table <- data.frame(t(asv_table))

#set column names to be ASV# 
colnames(asv_table) <- asv_table["ASV",]

#remove row that has ASV#
asv_table <- asv_table[!rownames(asv_table) %in% c('ASV'), ]

#make sure reads are numbers
# Convert all character columns to numeric
for (col in names(asv_table)) {
  asv_table[[col]] <- as.numeric(asv_table[[col]])
}

#make make sample ID a column 
asv_table$sample_ID_date <- rownames(asv_table)

#rename the one sample that got the wrong ID in the sample sheet 
asv_table <- asv_table %>%
  mutate(sample_ID_date = ifelse(sample_ID_date == "e0683-A-20240423", "e00683-A-20240423", sample_ID_date)) %>%
  mutate(sample_ID_date = ifelse(sample_ID_date == "e0683-B-20240423", "e00683-B-20240423", sample_ID_date)) %>%
  mutate(sample_ID_date = ifelse(sample_ID_date == "e0683-C-20240423", "e00683-C-20240423", sample_ID_date))
```


add column to the ASV table that labels the sample type
```{r}
asv_table_with_sample_type <- sample_metadata %>%
  dplyr::select(sample_ID_date, sample_type, collection_year, project, seq_date) %>%
  left_join(asv_table, by = "sample_ID_date") %>%
  unite(col = "project_year", project, collection_year, sep = "_", remove = F)

# make a variable for the first and last ASV column in the table
asv_first <- which(colnames(asv_table_with_sample_type) == "ASV_0001")
asv_last <- ncol(asv_table_with_sample_type)
```


# account for likely contaminants 

- tag-jumping: this would be a run-specific process, so would need to separate the data by sequencing run. 
- also when considering what's in the negative PCR controls, that should be done on by-run basis.
- field negatives should be addressed on a project/year basis. 


## Step 1. Account for tag-jumping by using the positive controls 

subtract the proportion of reads that jumped into the positive control samples from each environmental sample 

identify the maximum proportion of reads for each ASV found in the positive controls
```{r}
prop_asvs_in_positives_20240423 <- asv_table_with_sample_type %>%
  filter(seq_date == "20240423") %>%
  filter(sample_type == "positive") %>%
  pivot_longer(cols = c(asv_first:asv_last), names_to = "ASV", values_to = "reads") %>%
  group_by(sample_ID_date) %>%
  mutate(TotalReadsPerSample = sum(reads)) %>%
  mutate(Prop = reads/TotalReadsPerSample) %>%
  group_by(ASV) %>%
  summarise(max_prop = max(Prop)) %>%
  arrange(desc(max_prop)) %>%
  mutate(seq_date = 20240423)
prop_asvs_in_positives_20240423
```
```{r}
prop_asvs_in_positives_20240509 <- asv_table_with_sample_type %>%
  filter(seq_date == "20240509") %>%
  filter(sample_type == "positive") %>%
  pivot_longer(cols = c(asv_first:asv_last), names_to = "ASV", values_to = "reads") %>%
  group_by(sample_ID_date) %>%
  mutate(TotalReadsPerSample = sum(reads)) %>%
  mutate(Prop = reads/TotalReadsPerSample) %>%
  group_by(ASV) %>%
  summarise(max_prop = max(Prop)) %>%
  arrange(desc(max_prop)) %>%
  mutate(seq_date = 20240509)
prop_asvs_in_positives_20240509
```

```{r}
prop_asvs_in_positives_20240611 <- asv_table_with_sample_type %>%
  filter(seq_date == "20240611") %>%
  filter(sample_type == "positive") %>%
  pivot_longer(cols = c(asv_first:asv_last), names_to = "ASV", values_to = "reads") %>%
  group_by(sample_ID_date) %>%
  mutate(TotalReadsPerSample = sum(reads)) %>%
  mutate(Prop = reads/TotalReadsPerSample) %>%
  group_by(ASV) %>%
  summarise(max_prop = max(Prop)) %>%
  arrange(desc(max_prop)) %>%
  mutate(seq_date = 20240611)
prop_asvs_in_positives_20240611
```

```{r}
prop_asvs_in_positives_20240613 <- asv_table_with_sample_type %>%
  filter(seq_date == "20240613") %>%
  filter(sample_type == "positive") %>%
  pivot_longer(cols = c(asv_first:asv_last), names_to = "ASV", values_to = "reads") %>%
  group_by(sample_ID_date) %>%
  mutate(TotalReadsPerSample = sum(reads)) %>%
  mutate(Prop = reads/TotalReadsPerSample) %>%
  group_by(ASV) %>%
  summarise(max_prop = max(Prop)) %>%
  arrange(desc(max_prop)) %>%
  mutate(seq_date = 20240613)
prop_asvs_in_positives_20240613
```

combine these tables
```{r}
prop_asvs_in_positives <- prop_asvs_in_positives_20240423 %>%
  bind_rows(prop_asvs_in_positives_20240509) %>%
  bind_rows(prop_asvs_in_positives_20240611) %>%
  bind_rows(prop_asvs_in_positives_20240613)
```


subtract the max proportion of tag-jumped reads for each ASV from samples
```{r}
indexhop_table <- asv_table_with_sample_type %>%
  pivot_longer(cols = c(asv_first:asv_last), names_to = "ASV", values_to = "reads") %>%
  mutate(reads = ifelse(is.na(reads), 0, reads)) %>%
  group_by(sample_ID_date) %>%
  mutate(TotalReadsPerSample = sum(reads, na.rm = T)) %>%
  left_join(prop_asvs_in_positives, by = c("ASV", "seq_date")) %>%
  mutate(IndexHoppingReads = TotalReadsPerSample*max_prop) %>%
  mutate(reads_IndexHop_removed = reads - IndexHoppingReads) %>%
  mutate(reads_IndexHop_removed = if_else(reads_IndexHop_removed < 0, 0, reads_IndexHop_removed))
head(indexhop_table)
```

clean up the table by removing columns no longer needed 
```{r}
asv_table_filter1 <- indexhop_table %>%
  dplyr::select(sample_ID_date, sample_type, project_year, collection_year, project, seq_date, ASV, reads_IndexHop_removed) %>%
  dplyr::rename(reads = reads_IndexHop_removed)
```

this is a summary of the number of reads removed by ASV and sample_ID
```{r}
decontaminated_1 <- indexhop_table %>%
  dplyr::select(sample_ID_date, sample_type, project_year, collection_year, project, seq_date, ASV, IndexHoppingReads) %>%
  filter(sample_type == "sample") %>%
  group_by(seq_date, ASV) %>%
  summarise(mean_reads = mean(IndexHoppingReads),
            reads_q.05 = quantile(IndexHoppingReads, probs=0.05),
            median_q.5 = median(IndexHoppingReads),
            reads_q.95 = quantile(IndexHoppingReads, probs=0.95)) %>%
  filter(mean_reads > 0) %>%
  filter(ASV != "ASV_0014")  ## remove the PC/sturgeon ASV
decontaminated_1  
```


## Step 2. Account for contaminants in positive and negative controls 

next we will remove ASVs that only occur in controls and not in environmental samples. 

number of reads
```{r}
reads_per_type_ASV <- asv_table_filter1 %>%
  group_by(ASV, sample_type) %>%
  summarize(TotalReadsPerASV = sum(reads, na.rm = TRUE)) %>%
  arrange(ASV)
```

what ASVs have no reads in samples, but reads in the controls? 
```{r}
not_in_samples <- reads_per_type_ASV %>%
  pivot_wider(names_from = "sample_type", values_from = c("TotalReadsPerASV")) %>%
    filter(sample < 1)
not_in_samples
```


what ASVs do have reads in samples, but more reads in the controls? 
```{r}
more_in_pcr_blanks <- reads_per_type_ASV %>%
  pivot_wider(names_from = "sample_type", values_from = c("TotalReadsPerASV")) %>%
  filter(sample > 1) %>%
  filter(pcr_blank > sample)
head(more_in_pcr_blanks)

more_in_extraction_blanks <- reads_per_type_ASV %>%
  pivot_wider(names_from = "sample_type", values_from = c("TotalReadsPerASV")) %>%
  filter(sample > 1) %>%
  filter(extraction_blank > sample)
head(more_in_extraction_blanks)

more_in_pc_blanks <- reads_per_type_ASV %>%
  pivot_wider(names_from = "sample_type", values_from = c("TotalReadsPerASV")) %>%
  filter(sample > 1) %>%
  filter(positive > sample)
head(more_in_pc_blanks)

more_in_fb_blanks <- reads_per_type_ASV %>%
  pivot_wider(names_from = "sample_type", values_from = c("TotalReadsPerASV")) %>%
  filter(sample > 1) %>%
  filter(field_blank > sample)
head(more_in_fb_blanks)
```


remove these from the asv table
```{r}
asv_table_filter2 <- asv_table_filter1 %>%
  filter(!ASV %in% not_in_samples$ASV) %>%
  filter(!ASV %in% more_in_pcr_blanks$ASV) %>%
  filter(!ASV %in% more_in_extraction_blanks$ASV) %>%
  filter(!ASV %in% more_in_pc_blanks$ASV) %>%
  filter(!ASV %in% more_in_fb_blanks$ASV)
```


## Step 3. Remove ASVs without taxonomic ID 

now lets see how many of these ASVs have taxonomic IDs (these are not final tax ids)
```{r}
taxonomy <- read.csv("/home/kimberly.ledger/BeringSea_Arctic_eDNA/outputs/taxonomy_20240709.csv") %>%
  select(!X) %>% 
  mutate(taxon = ifelse(ASV == "ASV_0898", "Hippoglossus stenolepis", taxon)) %>%  ### manually fix a couple entries 
  mutate(taxon = ifelse(ASV == "ASV_1476", "Hippoglossoides", taxon)) %>%
  mutate(taxonomic_level = ifelse(ASV == "ASV_1476", "genus", taxonomic_level)) %>%
  unique()

missing_tax <- data.frame(ASV = c("ASV_0016"), taxon = "Mallotus villosus", taxonomic_level = "species")   ### need to go back and fix this in the taxonomy output at some point.. 

taxonomy <- taxonomy %>%
  bind_rows(missing_tax)
```

```{r}
asv_table_filter2_with_tax <- asv_table_filter2 %>%
  left_join(taxonomy)

# # Count occurrences of each key in table 1
# asv_table_filter2_counts <- asv_table_filter2 %>%
#   group_by(ASV) %>%
#   summarise(count = n())
# 
# # Count occurrences of each key in table 2
# taxonomy_counts <- taxonomy %>%
#   group_by(ASV) %>%
#   summarise(count = n())
# 
# # Identify keys that appear more than once in both tables
# many_to_many_keys <- inner_join(asv_table_filter2_counts %>% filter(count > 1),
#                                 taxonomy_counts %>% filter(count > 1),
#                                 by = "ASV")
# 
# # Print the keys with many-to-many relationships
# if (nrow(many_to_many_keys) > 0) {
#   print("Many-to-many relationships detected for the following keys:")
#   print(many_to_many_keys)
# } else {
#   print("No many-to-many relationships detected.")
# }
```

what ASV's do not have a taxonomic ID? 
```{r}
asv_table_filter2_with_tax %>%
  filter(is.na(taxon)) %>%
  group_by(ASV) %>%
  summarize(total_reads = sum(reads, na.rm = T))
```

28 is human. 
33 is Gadus chalcogrammus  ### TO DO - figure out why this was removed. 
50 is Oncorhynchus keta 
61 is Bos 

NEED TO DOUBLE CHECK THESE!!!  some ASVs seem like they should be keepers. 


remove ASVs with no taxonomic id
```{r}
asv_table_filter3 <- asv_table_filter2_with_tax %>%
  filter(!is.na(taxon))
```


## Step 4. Consider what is still in the negative controls - on a run-by-run basis

```{r}
asv_table_filter3 %>%
  filter(sample_type == "pcr_blank") %>%
  ggplot(aes(x=sample_ID_date, y=reads, fill=taxon)) +
  geom_bar(stat = "identity") + 
  theme_bw() +
  facet_grid(~seq_date, scales = "free_x") + 
  labs(
    y = "number of sequencing reads",
    x = "sample ID",
    title = "ASV reads - pcr blanks") + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.text = element_text(size = 8),
    legend.key.size = unit(0.3, "cm"),
    legend.position = "none",
    legend.title = element_blank()
  )
```

keep this at the asv level
```{r}
asvs_PCRN <- asv_table_filter3 %>%
  filter(sample_type == "pcr_blank") %>%
  group_by(seq_date, ASV) %>%
  summarise(total = sum(reads),
            max = max(reads),
            mean = mean(reads)) %>%
  arrange(desc(total)) %>%
  filter(total > 0)

asvs_PCRN
```

okay, so subtracting the max reads in the pcr negative would remove quite a few pink salmon (asv 13) occurrences from 20240423 run.  probably don't want to use that approach.  

how about using mean read count per ASV in the negative controls as the cutoff value

```{r}
asvs_PCRN_mean <- asvs_PCRN %>%
  select(!total) %>%
  select(!max)
  
pcrn_table <- asv_table_filter3 %>%
  left_join(asvs_PCRN_mean, by = c("seq_date", "ASV")) %>%
  mutate(mean = ifelse(is.na(mean), 0, mean)) %>%
  mutate(reads_pcrn_removed = reads - mean) %>%
  mutate(reads_pcrn_removed = if_else(reads_pcrn_removed < 0, 0, reads_pcrn_removed))
pcrn_table
```

clean up the table by removing columns no longer needed 
```{r}
asv_table_filter4 <- pcrn_table %>%
  select(!reads) %>%
  select(!mean) %>%
  dplyr::rename(reads = reads_pcrn_removed)
```


## Step 5.  Address field negatives. 

- field negatives should be addressed on a project/year basis.

plot by project and year 
```{r}
asv_table_filter4 %>%
  filter(sample_type == "field_blank") %>%
  ggplot(aes(x=sample_ID_date, y=reads, fill=taxon)) +
  geom_bar(stat = "identity") + 
  theme_bw() +
  facet_grid(~project_year, scales = "free_x") + 
  labs(
    y = "number of sequencing reads",
    x = "sample ID",
    title = "ASV reads - field blanks") + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.text = element_text(size = 8),
    legend.key.size = unit(0.3, "cm"),
    legend.position = "none",
    legend.title = element_blank()
  )
```

look closer at DBO 2023
```{r}
asv_table_filter4 %>%
  separate(sample_ID_date, into = c("extraction_ID", "replicate", "seq_date2"),  remove = F) %>%
  filter(sample_type == "field_blank") %>%
  filter(project_year == "DBO_2023") %>%
  filter(reads > 0) %>%
  ggplot(aes(x=sample_ID_date, y=reads, fill=taxon)) +
  geom_bar(stat = "identity") + 
  facet_grid(~extraction_ID, scales = "free_x") + 
  theme_bw() +
  labs(
    y = "number of sequencing reads",
    x = "sample ID",
    title = "DBO 2023 - field blanks") + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.text = element_text(size = 8),
    legend.key.size = unit(0.3, "cm"),
    legend.position = "right",
    legend.title = element_blank()
  )
```

three pcr reps of each field blank extraction. only field blank has lots of seqs in all three pcr reps. will need to report. 


look closer at NBS 2021 
```{r}
asv_table_filter4 %>%
  separate(sample_ID_date, into = c("extraction_ID", "replicate", "seq_date2"),  remove = F) %>%
  filter(sample_type == "field_blank") %>%
  filter(project_year == "NBS_2021") %>%
  filter(reads > 0) %>%
  ggplot(aes(x=sample_ID_date, y=reads, fill=taxon)) +
  geom_bar(stat = "identity") + 
  facet_grid(~extraction_ID, scales = "free_x") + 
  theme_bw() +
  labs(
    y = "number of sequencing reads",
    x = "sample ID",
    title = "NBS 2021 - field blanks") + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.text = element_text(size = 8),
    legend.key.size = unit(0.3, "cm"),
    legend.position = "right",
    legend.title = element_blank()
  )
```


look closer at NBS 2022 
```{r}
asv_table_filter4 %>%
  separate(sample_ID_date, into = c("extraction_ID", "replicate", "seq_date2"),  remove = F) %>%
  filter(sample_type == "field_blank") %>%
  filter(project_year == "NBS_2022") %>%
  filter(reads > 0) %>%
  ggplot(aes(x=sample_ID_date, y=reads, fill=taxon)) +
  geom_bar(stat = "identity") + 
  facet_grid(~extraction_ID, scales = "free_x") + 
  theme_bw() +
  labs(
    y = "number of sequencing reads",
    x = "sample ID",
    title = "NBS 2022 - field blanks") + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.text = element_text(size = 8),
    legend.key.size = unit(0.3, "cm"),
    legend.position = "right",
    legend.title = element_blank()
  )
```


look closer at NBS 2023 
```{r}
asv_table_filter4 %>%
  separate(sample_ID_date, into = c("extraction_ID", "replicate", "seq_date2"),  remove = F) %>%
  filter(sample_type == "field_blank") %>%
  filter(project_year == "NBS_2023") %>%
  filter(reads > 0) %>%
  ggplot(aes(x=sample_ID_date, y=reads, fill=taxon)) +
  geom_bar(stat = "identity") + 
  facet_grid(~extraction_ID, scales = "free_x") + 
  theme_bw() +
  labs(
    y = "number of sequencing reads",
    x = "sample ID",
    title = "NBS 2023 - field blanks") + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.text = element_text(size = 8),
    legend.key.size = unit(0.3, "cm"),
    legend.position = "right",
    legend.title = element_blank()
  )
```


look closer at SBS 2022 
```{r}
asv_table_filter4 %>%
  separate(sample_ID_date, into = c("extraction_ID", "replicate", "seq_date2"),  remove = F) %>%
  filter(sample_type == "field_blank") %>%
  filter(project_year == "SBS_2022") %>%
  filter(reads > 0) %>%
  ggplot(aes(x=sample_ID_date, y=reads, fill=taxon)) +
  geom_bar(stat = "identity") + 
  facet_grid(~extraction_ID, scales = "free_x") + 
  theme_bw() +
  labs(
    y = "number of sequencing reads",
    x = "sample ID",
    title = "SBS 2022 - field blanks") + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.text = element_text(size = 8),
    legend.key.size = unit(0.3, "cm"),
    legend.position = "right",
    legend.title = element_blank()
  )
```


alrighty, depending on how i end up handling field samples, the high dissimilarity and single pcr reps with seqs from field blanks may be able to stand out from the real samples.  


```{r}
#write.csv(asv_table_filter4, "/home/kimberly.ledger/BeringSea_Arctic_eDNA/outputs/decontaminated_20240710.csv")
```


## Step 6. explore the field samples 
- use species accumulation curves with respect to the number of corrected reads to explore if that the sequencing depth was sufficient to detect all of the species contained in a sample


but first let's check how many pcr reps each extraction has 

are there any samples that have made it to this point that don't actually have any reads? 
```{r}
ZeroReadCount <- asv_table_filter4 %>%
  group_by(sample_ID_date, sample_type) %>%
  summarise(total_reads = sum(reads)) %>%
  filter(total_reads == 0) %>%
  group_by(sample_type) %>%
  summarize(total = n())
ZeroReadCount 
```

quite a few negative and also many samples without any sequencing reads 

```{r}
asv_table_filter5 <- asv_table_filter4 %>% 
  filter(!sample_ID_date %in% ZeroReadCount$sample_ID_date) %>%
  separate(sample_ID_date, into = c("extraction_ID", "rep", "seq_date2"), sep = "-", remove = F)
```

how many extractions (excluding the ones with zero reads) have only one pcr replicate? 
```{r}
OneRep <- asv_table_filter5 %>%
  group_by(extraction_ID, sample_type) %>%
  summarise(nrep = n_distinct(sample_ID_date)) %>%
  filter(nrep == 1) %>%
  group_by(sample_type) %>%
  summarize(total = n())
OneRep
```

how many extractions have two pcr replicates? 
```{r}
TwoRep <- asv_table_filter5 %>%
  group_by(extraction_ID, sample_type) %>%
  summarise(nrep = n_distinct(sample_ID_date)) %>%
  filter(nrep == 2) %>%
  group_by(sample_type) %>%
  summarize(total = n())  
TwoRep
```

how many extractions have three pcr replicates? 
```{r}
ThreeRep <- asv_table_filter5 %>%
  group_by(extraction_ID, sample_type) %>%
  summarise(nrep = n_distinct(sample_ID_date)) %>%
  filter(nrep >= 3) %>%
  group_by(sample_type) %>%
  summarize(total = n())  
ThreeRep
```


how do these numbers change if i set a pretty low read count per pcr replicate threshold to filter? 

```{r}
reads_per_pcr_rep <- asv_table_filter5 %>%
  group_by(sample_ID_date, sample_type) %>%
  summarise(tot_reads = sum(reads))   ##1737

atleast_500 <- reads_per_pcr_rep %>%
  filter(tot_reads >= 500)            ##1350
```

```{r}
asv_table_filter6 <- asv_table_filter5 %>%
  filter(sample_ID_date %in% atleast_500$sample_ID_date)
```

how many extractions (excluding the ones with zero reads) have only one pcr replicate? 
```{r}
OneRep <- asv_table_filter6 %>%
  group_by(extraction_ID, sample_type) %>%
  summarise(nrep = n_distinct(sample_ID_date)) %>%
  filter(nrep == 1) %>%
  group_by(sample_type) %>%
  summarize(total = n())
OneRep
```

how many extractions have two pcr replicates? 
```{r}
TwoRep <- asv_table_filter6 %>%
  group_by(extraction_ID, sample_type) %>%
  summarise(nrep = n_distinct(sample_ID_date)) %>%
  filter(nrep == 2) %>%
  group_by(sample_type) %>%
  summarize(total = n())  
TwoRep
```

how many extractions have three pcr replicates? 
```{r}
ThreeRep <- asv_table_filter6 %>%
  group_by(extraction_ID, sample_type) %>%
  summarise(nrep = n_distinct(sample_ID_date)) %>%
  filter(nrep >= 3) %>%
  group_by(sample_type) %>%
  summarize(total = n())  
ThreeRep
```

just to remind myself, how many extractions from samples should there be if everything was perfect? 
```{r}
sample_metadata %>%
  filter(sample_type == "sample") %>%
  select(extraction_ID) %>%
  unique() %>%
  summarize(count = n())
```

okay, so with decontamination and no read filter it looks like 467+19 = 486 out of the 489 field samples have two or more pcr replicates with reads. 
after filtering out pcr reps with <500 reads, we are down to 348+68 =  416 out of the 489 field samples have two or more pcr replicates with reads.

read in some metadata that groups extractions IDs into sites 
```{r}
metadata <- read.csv("/home/kimberly.ledger/BeringSea_Arctic_eDNA/data/NBS_SBS_DBO_metadata.csv") 

metadata_mini <- metadata %>%
  select(extraction_ID, project, collection_year, location1)  ## this set up doesn't work for DBO sample format right now - will need to fix 
```


what are some of these samples with < 500 reads?  let me pull them out on the accumulation curve. 
```{r}
lessthan_500 <- reads_per_pcr_rep %>%
  filter(sample_type == "sample") %>%
  filter(tot_reads < 500) %>%
  separate(sample_ID_date, into = c("extraction_ID", "rep", "seq_date2"), sep = "-", remove = F) %>%
  left_join(metadata_mini)
```


now, let's get to the accumulation curve thing. will start by running this by pcr reps and then can also try by extraction IDs. 

iNext seems to crash because of too many samples.  let me break this up by project and year to see if it helps.
```{r}
NBS_2021 <- asv_table_filter5 %>%
  filter(project_year == "NBS_2021")

NBS_2022 <- asv_table_filter5 %>%
  filter(project_year == "NBS_2022")

SBS_2022 <- asv_table_filter5 %>%
  filter(project_year == "SBS_2022")
```

okay, still crashes with just NBS 2021. let's approach this another way.

```{r}
NBS_2021_w_meta <- NBS_2021 %>%
  left_join(metadata_mini)

NBS_2022_w_meta <- NBS_2022 %>%
  left_join(metadata_mini)

SBS_2022_w_meta <- SBS_2022 %>%
  left_join(metadata_mini)
```


try out accumulation curve to evaluate sequencing depth 
```{r}
library(iNEXT)
library(ggplot2)

#maybe "abundance" will be the easiest to format?
# need N lists of species read counts 

taxon_table <- SBS_2022_w_meta %>%
  filter(sample_type == "sample") %>%       ##just going to do this for real samples for now. 
   group_by(sample_ID_date, location1, taxon) %>%
   summarize(total_reads = sum(reads)) %>%
   pivot_wider(names_from = taxon, values_from = total_reads)
taxon_table

# now convert the taxon table to lists of species abundances (one list per pcr rep)

# okay, need to go back to long format 
taxon_long <- taxon_table %>%
  pivot_longer(cols = c(3:114), names_to = "taxon", values_to = "reads") %>%    #### CHECK COLUMN VALUES!!!
  filter(reads > 0) %>% # filter out entries with no reads
  arrange(sample_ID_date, desc(reads))  # sort reads in descending order and group by sample

st <- taxon_long %>%
  filter(location1 == "20")

# Nest the data to create a list of species reads for each sample
read_lists <- st %>%
  group_by(sample_ID_date) %>%
  nest() %>%
  mutate(
    species_read_list = map(data, ~ .x %>% select(taxon, reads))
  ) %>%
  select(sample_ID_date, species_read_list)

#Convert to a list of lists
read_list_final <- read_lists %>%
  pull(species_read_list) %>%
  setNames(read_lists$sample_ID_date)

# Convert each table to a numeric vector of abundances
read_lists_vector <- map(read_list_final, ~ {
  .x %>%
    filter(reads > 0) %>%
    arrange(desc(reads)) %>%
    pull(reads)
})

# subset to test how it works 
#mini_list <- head(read_lists_vector, 8)

# try to run iNEXT - the function doesn't like non-integers so lots of warning will pop up.
#out_mini <- iNEXT(mini_list, q = 0, datatype = "abundance")
#out_nbs2021_st5 <- iNEXT(read_lists_vector, q = 0, datatype = "abundance")
#out_nbs2021_st29 <- iNEXT(read_lists_vector, q = 0, datatype = "abundance")
#out_nbs2021_st23 <- iNEXT(read_lists_vector, q = 0, datatype = "abundance")
#out_nbs2022_st7 <- iNEXT(read_lists_vector, q = 0, datatype = "abundance")
#out_nbs2022_st16 <- iNEXT(read_lists_vector, q = 0, datatype = "abundance")
#out_nbs2022_st37 <- iNEXT(read_lists_vector, q = 0, datatype = "abundance")
#out_sbs2022_st16 <- iNEXT(read_lists_vector, q = 0, datatype = "abundance")
out_sbs2022_st20 <- iNEXT(read_lists_vector, q = 0, datatype = "abundance")

```

check out the output 
```{r}
out_nbs2021$DataInfo
out_nbs2021$iNextEst
out_nbs2021$AsyEst
```
 
plot the output  - i'm getting color plot errors when there are more then 8 
```{r}
# Assuming 'out' is your iNEXT object
#unique_assemblages <- length(unique(out$DataInfo$Assemblage))

#library(RColorBrewer)
# Generate a color palette with the required number of colors
#palette <- brewer.pal(n = unique_assemblages, name = "Set3")

#ggiNEXT(out_nbs2021_st5, type = 1, color.var = "Assemblage") #+ 
  #scale_colour_manual(values = palette)
```
 
custom plot since ggiNEXT is giving me color problems 
```{r}
df <- fortify(out_sbs2022_st20, type=1)
df.point <- df[which(df$Method=="Observed"),]
df.line <- df[which(df$Method!="Observed"),]
df.line$Method <- factor(df.line$Method,  c("Rarefaction", "Extrapolation"), c("Rarefaction", "Extrapolation"))

my_plot <- ggplot(df, aes(x=x, y=y, colour=Assemblage)) +  
  geom_point(size=5, data=df.point) + 
  geom_line(aes(linetype=Method), lwd=1.5, data=df.line) + 
  geom_ribbon(aes(ymin=y.lwr, ymax=y.upr, fill=Assemblage, colour=NULL), alpha=0.2) + 
  labs(x="Number of sequencing reads", y="Number of detected taxa", 
       title = "SBS 2022 - st 20") + 
  theme(legend.position = "bottom",  legend.title=element_blank(), text=element_text(size=18), legend.box = "vertical") +
  #xlim(0, 4000) + 
  theme_minimal()
my_plot
```
 
```{r}
ggsave("/home/kimberly.ledger/BeringSea_Arctic_eDNA/outputs/accumulation_curves/SBS_2022_st20.png", plot = my_plot, dpi = 150, width = 8, height = 6)
```
 


I'M NOT SURE THIS CODE IS CORRECT - got it from chatgpt
okay, the plateau seems to be around 1000 reads - let's see if i can actually quantify that 
```{r}
# Extract the data for one of the assemblages
data_curve <- out_nbs2021_st5$iNextEst$size_based[out_nbs2021_st5$iNextEst$size_based$Assemblage == "e00614-C-20240423",]
#data_curve <- out_nbs2022_st16$iNextEst$size_based[out_nbs2022_st16$iNextEst$size_based$Assemblage == "e01855-A-20240423",]

# Fit a smoothing spline
fit <- smooth.spline(data_curve$m, data_curve$qD)

# Compute the derivative
deriv <- predict(fit, data_curve$m, deriv = 1)

# Plot the derivative to find where it approaches zero
plot(data_curve$m, deriv$y, type = "l", main = "First Derivative of Accumulation Curve", xlab = "Sample Size", ylab = "Rate of Change")

# Identify the plateau point
threshold <- 0.01  # Define a threshold for the slope
plateau_index <- which(abs(deriv$y) < threshold)[1]
plateau_point <- data_curve$m[plateau_index]

print(paste("The curve begins to plateau at sample size:", plateau_point))
```


determine if observed data point is on plateau. 

COME BACK TO THIS ANALYSIS


## next closer look at SBS data
-compare samples with multiple library preps
- can we combine? just use one or the other??? 



 



 
 