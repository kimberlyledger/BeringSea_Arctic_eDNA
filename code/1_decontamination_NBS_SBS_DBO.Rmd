---
title: "Secondary QC and Decontamination"
author: "Kimberly Ledger"
date: "2024-06-18"
output: html_document
---

Dataset: NBS 2021/22/23, SBS 2022, and DBO 2021/23 eDNA w/ MiFish


```{r setup, include=FALSE}
knitr::opts_chunk$set(root.dir = "/home/kimberly.ledger/BeringSea_Arctic_eDNA/")
```

Inputs: This code starts with the ASV table output from dadasnake.sh that uses dadasnake for preliminary quality control.

load libraries 
```{r, message=FALSE}
library(tidyverse)
library(dplyr)
library(rstan)
library(broom)
library(vegan)
library(reshape)
```

load ASV table and metadata
```{r}
asv_table <- readRDS("/home/kimberly.ledger/BeringSea_Arctic_eDNA/data/dadasnake/filtered.seqTab.RDS") %>%
  select(!Row.names)

#transpose 
asv_table <- data.frame(t(asv_table))

#set column names to be ASV# 
colnames(asv_table) <- asv_table["ASV",]

#remove row that has ASV#
asv_table <- asv_table[!rownames(asv_table) %in% c('ASV'), ]

#make sure reads are numbers
# Convert all character columns to numeric
for (col in names(asv_table)) {
  asv_table[[col]] <- as.numeric(asv_table[[col]])
}

#make make sample ID a column 
asv_table$sample_ID <- rownames(asv_table)

#rename the one sample that got the wrong ID in the sample sheet 
asv_table <- asv_table %>%
  mutate(sample_ID = ifelse(sample_ID == "e0683-A", "e00683-A", sample_ID)) %>%
  mutate(sample_ID = ifelse(sample_ID == "e0683-B", "e00683-B", sample_ID)) %>%
  mutate(sample_ID = ifelse(sample_ID == "e0683-C", "e00683-C", sample_ID))  

#asv_table$sample_ID <- as.factor(asv_table$sample_ID)

#read in some sample metadata to work with 
sample_metadata <- read.csv("/home/kimberly.ledger/BeringSea_Arctic_eDNA/data/sample_names_NBS_SBS_DBO.csv")

#illumina output changed "_" to "-"
sample_metadata$sample_ID <- gsub("_", "-", sample_metadata$sample_ID)
```

for now remove the SBS22 that were duplicated from the 20230423 miseq run from the sample metadata
```{r}
discard <- data.frame(project = c("SBS"), extraction_plate = c("16_2023"), seq_date = c(20240423))
  
sample_metadata <- sample_metadata %>%
  anti_join(discard, by = c("project", "extraction_plate", "seq_date"))
```


add column to the ASV table that labels the sample type
```{r}
asv_table_with_sample_type <- sample_metadata %>%
  dplyr::select(sample_ID, sample_type, collection_year, project) %>%
  left_join(asv_table, by = "sample_ID")

# make a variable for the first and last ASV column in the table
asv_first <- which(colnames(asv_table_with_sample_type) == "ASV_0001")
asv_last <- ncol(asv_table_with_sample_type)
```


### positive controls 

let's start by visualizing the reads in the positive control samples 
```{r}
asv_table_with_sample_type %>%
  pivot_longer(cols = c(asv_first:asv_last), names_to = "ASV", values_to = "reads") %>%
  filter(sample_type == "positive") %>%
  ggplot(aes(x=sample_ID, y=reads, fill=ASV)) +
  geom_bar(stat = "identity") +
  facet_grid(project~collection_year, scales = "free_x") + 
  theme_bw() +
  labs(
    y = "number of sequencing reads",
    x = "sample ID",
    title = "ASV reads in positive controls") + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.text = element_text(size = 8),
    legend.key.size = unit(0.3, "cm"),
    legend.position = "none",
    legend.title = element_blank()
  )
```

top asvs in positive controls
```{r}
asvs_PC <- asv_table_with_sample_type %>%
  pivot_longer(cols = c(asv_first:asv_last), names_to = "ASV", values_to = "reads") %>%
  filter(sample_type == "positive") %>%
  group_by(ASV) %>%
  summarise(total = sum(reads)) %>%
  arrange(desc(total))

head(asvs_PC, 6)
```

okay, so ASV_0014 must be the sturgeon (and maybe ASV_0264?)

### extraction blanks 

let me look into the reads that got into the extraction blanks
```{r}
asv_table_with_sample_type %>%
  pivot_longer(cols = c(asv_first:asv_last), names_to = "ASV", values_to = "reads") %>%
  filter(sample_type == "extraction_blank") %>%
  ggplot(aes(x=sample_ID, y=reads, fill=ASV)) +
  facet_grid(project~collection_year, scales = "free_x") +  
  geom_bar(stat = "identity") + 
  theme_bw() +
  labs(
    y = "number of sequencing reads",
    x = "sample ID",
    title = "ASV reads - extraction blanks") + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.text = element_text(size = 8),
    legend.key.size = unit(0.3, "cm"),
    legend.position = "none",
    legend.title = element_blank()
  )
```

all extraction blanks associated with NBS samples, id's in the 600's are associated with 2021 and 1800's are associated with 2022. 

```{r}
asvs_EC <- asv_table_with_sample_type %>%
  pivot_longer(cols = c(asv_first:asv_last), names_to = "ASV", values_to = "reads") %>%
  filter(sample_type == "extraction_blank") %>%
  group_by(ASV, project, collection_year) %>%
  summarise(total = sum(reads)) %>%
  arrange(desc(total))

head(asvs_EC, 10)
```

### pcr blanks 

let me look into the reads that got into the pcr blanks
```{r}
asv_table_with_sample_type %>%
  pivot_longer(cols = c(asv_first:asv_last), names_to = "ASV", values_to = "reads") %>%
  filter(sample_type == "PCR_blank" | sample_type == "PCR_control") %>%
  ggplot(aes(x=Sample_ID, y=reads, fill=ASV)) +
  geom_bar(stat = "identity") + 
    theme_bw() +
  labs(
    y = "number of sequencing reads",
    x = "sample ID",
    title = "ASV reads - pcr negatives") + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.text = element_text(size = 8),
    legend.key.size = unit(0.3, "cm"),
    legend.position = "none",
    legend.title = element_blank()
  )
```

okay some substantial contamination in the PCR negatives associated with plate 1 (NBS 2021 samples)

```{r}
asvs_PCRN <- asv_table_with_sample_type %>%
  pivot_longer(cols = c(asv_first:asv_last), names_to = "ASV", values_to = "reads") %>%
  filter(sample_type == "PCR_blank" | sample_type == "PCR_control") %>%
  group_by(ASV) %>%
  summarise(total = sum(reads)) %>%
  arrange(desc(total))

head(asvs_PCRN, 10)
```

### field blanks 

let me look into the reads that got into the pcr blanks
```{r}
asv_table_with_sample_type %>%
  pivot_longer(cols = c(asv_first:asv_last), names_to = "ASV", values_to = "reads") %>%
  filter(sample_type == "field_blank") %>%
  ggplot(aes(x=Sample_ID, y=reads, fill=ASV)) +
  geom_bar(stat = "identity") + 
  facet_grid(project~collection_year, scales = "free_x") + 
  theme_bw() +
  labs(
    y = "number of sequencing reads",
    x = "sample ID",
    title = "ASV reads - field negatives") + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.text = element_text(size = 8),
    legend.key.size = unit(0.3, "cm"),
    legend.position = "none",
    legend.title = element_blank()
  )
```

okay, so signification contamination in 2021 NBS samples. not so much in 2022 NBS/SBS samples 

```{r}
# for an unknown reason sample e01865-A is missing from the ASV table output. having NA's messes up the sum function so i'm going to just remove that sample 

asv_table_with_sample_type <- asv_table_with_sample_type %>%
  filter(Sample_ID != "e01865-A")

asvs_FC <- asv_table_with_sample_type %>%
  pivot_longer(cols = c(asv_first:asv_last), names_to = "ASV", values_to = "reads") %>%
  filter(sample_type == "field_blank") %>%
  group_by(ASV, collection_year, project) %>%
  summarise(total = sum(reads)) %>%
  arrange(desc(total))

head(asvs_FC, 10)
```

eek. lots of reads in field blanks. especially from 2021 NBS. 

quick look at field samples 
```{r}
asv_table_with_sample_type %>%
  pivot_longer(cols = c(asv_first:asv_last), names_to = "ASV", values_to = "reads") %>%
  filter(sample_type == "sample") %>%
  ggplot(aes(x=Sample_ID, y=reads, fill=ASV)) +
  geom_bar(stat = "identity") + 
  facet_grid(project~collection_year, scales = "free_x") + 
  theme_bw() +
  labs(
    y = "number of sequencing reads",
    x = "sample ID",
    title = "ASV reads - field samples") + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.text = element_text(size = 8),
    legend.key.size = unit(0.3, "cm"),
    legend.position = "none",
    legend.title = element_blank()
  )
```


# Now to the decontamination steps 

## 1. Estimate index hopping  
subtract the proportion of reads that jumped into the positive control samples from each environmental sample 

identify the maximum proportion of reads for each ASV found in the positive controls
```{r}
prop_asvs_in_positives <- asv_table_with_sample_type %>%
  filter(sample_type == "positive_control") %>%
  pivot_longer(cols = c(asv_first:asv_last), names_to = "ASV", values_to = "reads") %>%
  group_by(Sample_ID) %>%
  mutate(TotalReadsPerSample = sum(reads)) %>%
  mutate(Prop = reads/TotalReadsPerSample) %>%
  group_by(ASV) %>%
  summarise(max_prop = max(Prop)) %>%
  arrange(desc(max_prop))
prop_asvs_in_positives
```

tag-jumping is not an issue with this dataset so i am not going to do anything here. but will need to remove all reads belonging to ASV8 (the PC) at some point

## 2. Account for contaminants in positive and negative controls 

next we will remove ASVs that only occur in controls and not in environmental samples. 

let's start by taking a look at what reads remain in these controls 
```{r}
asv_table_with_sample_type <- asv_table_with_sample_type %>%
  mutate(sample_type = ifelse(sample_type == "PCR_control", "PCR_blank", sample_type))

asv_table_with_sample_type %>%
  pivot_longer(cols = c(asv_first:asv_last), names_to = "ASV", values_to = "reads") %>%
  filter(sample_type != "sample") %>%
  ggplot(aes(x=Sample_ID, y=reads, fill=ASV)) +
  geom_bar(stat = "identity") + 
  facet_grid(~sample_type, scales = "free_x") + 
  theme_bw() +
  labs(
    y = "number of sequencing reads",
    x = "sample ID",
    title = "ASV reads - controls") + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.text = element_text(size = 8),
    legend.key.size = unit(0.3, "cm"),
    legend.position = "none",
    legend.title = element_blank()
  )
```

number of reads
```{r}
reads_per_type_ASV <- asv_table_with_sample_type %>%
  pivot_longer(cols = c(asv_first:asv_last), names_to = "ASV", values_to = "reads") %>%
  group_by(ASV, sample_type) %>%
  summarize(TotalReadsPerASV = sum(reads)) %>%
  arrange(ASV)
```

what ASVs have no reads in samples, but reads in the controls? 
```{r}
not_in_samples <- reads_per_type_ASV %>%
  pivot_wider(names_from = "sample_type", values_from = c("TotalReadsPerASV")) %>%
    filter(sample < 1)
head(not_in_samples)
```

asv53 = 100% nile or mozambique tilapia 
asv73 = 99.4% pig 
asv86 = 98.9% lake sturgeon (variant asv of positive control)


what ASVs have reads in samples, but more reads in the controls? 
```{r}
more_in_pcr_blanks <- reads_per_type_ASV %>%
  pivot_wider(names_from = "sample_type", values_from = c("TotalReadsPerASV")) %>%
  filter(sample > 1) %>%
  filter(PCR_blank > sample)
head(more_in_pcr_blanks)

more_in_extraction_blanks <- reads_per_type_ASV %>%
  pivot_wider(names_from = "sample_type", values_from = c("TotalReadsPerASV")) %>%
  filter(sample > 1) %>%
  filter(extraction_blank > sample)
head(more_in_extraction_blanks)

more_in_pc_blanks <- reads_per_type_ASV %>%
  pivot_wider(names_from = "sample_type", values_from = c("TotalReadsPerASV")) %>%
  filter(sample > 1) %>%
  filter(positive_control > sample)
head(more_in_pc_blanks)

more_in_fb_blanks <- reads_per_type_ASV %>%
  pivot_wider(names_from = "sample_type", values_from = c("TotalReadsPerASV")) %>%
  filter(sample > 1) %>%
  filter(field_blank > sample)
head(more_in_fb_blanks)
```


okay, want to remove asv8 since it's the pc. 
what about asvs that are greater in field blanks?   
asv58 = Hexagrammos sp. 
asv63 = Northern fur seal 
asv68 = 99.4% herring 

relatively low numbers of reads of these asvs in samples, so the conservative approach is to remove them 


remove these from the data frame 
```{r}
asv_table_filter1 <- asv_table_with_sample_type %>%
  pivot_longer(cols = c(asv_first:asv_last), names_to = "ASV", values_to = "reads") %>%
  filter(!ASV %in% not_in_samples$ASV) %>%
  filter(!ASV %in% more_in_pc_blanks$ASV) %>%
  filter(!ASV %in% more_in_fb_blanks$ASV)
```


how much does this change things in the controls?
```{r}
asv_table_filter1 %>%
  filter(sample_type != "sample") %>%
  ggplot(aes(x=Sample_ID, y=reads, fill=ASV)) +
  geom_bar(stat = "identity") + 
  facet_grid(~sample_type, scales = "free_x") + 
  theme_bw() +
  labs(
    y = "number of sequencing reads",
    x = "sample ID",
    title = "ASV reads - controls") + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.text = element_text(size = 8),
    legend.key.size = unit(0.3, "cm"),
    legend.position = "none",
    legend.title = element_blank()
  )
```

okay, that helped but reads in some extraction/pcr blanks and lots in the field blanks.  


next we will subtract the maximum number of reads from ASVs found in the extraction and pcr negative controls from all samples.

calculate the maximum number of reads in an ASV to still show up in an extraction or PCR negative control 
```{r}
reads_to_remove_per_ASV <- asv_table_filter1 %>%
  filter(sample_type == "extraction_blank"| sample_type == "PCR_blank") %>%
  group_by(ASV) %>%
  summarize(max_reads = max(reads)) %>%
  arrange(desc(max_reads))

#asv9 - 100% pink salmon; 99.4% sockeye salmon - ~3100 reads
#asv2 - 100% herring - ~2300 reads 


## does it make sense to remove the salmon and herring reads from samples??? 

asv_table_filter1 %>% 
  filter(sample_type == "sample") %>%
  filter(ASV == "ASV_0009" | ASV == "ASV_0002") %>%
  group_by(ASV, project, collection_year) %>%
  summarize(mean = mean(reads),
                SD=sd(reads),
                q.025 = quantile(reads,probs=0.025),
                Median = quantile(reads,probs=0.5),
                q.975 = quantile(reads,probs=0.975))

# for asv2 (herring), subtracting reads would eliminate the asv from SBS 2022 samples, and just reduce reads in NBS 2021 and NBS 2022 samples
# for asv9 (pink salmon), subtracting reads would eliminate the asv from nearly all samples 

reads_to_remove_per_sample <- asv_table_filter1 %>%
  left_join(reads_to_remove_per_ASV, by = "ASV") %>%
  mutate(read_minus_contamination = reads - max_reads) %>%
  mutate(read_minus_contamination = if_else(read_minus_contamination < 0, 0, read_minus_contamination))
```


need to make a decision on whether it's appropriate to subtract reads from samples at some point, for now i will keep them. 


now let's consider the field blanks in more detail. 
```{r}
asv_table_filter1 %>%
  filter(sample_type == "field_blank") %>%
  ggplot(aes(x=Sample_ID, y=reads, fill=ASV)) +
  geom_bar(stat = "identity") + 
  facet_grid(project~collection_year, scales = "free_x") + 
  theme_bw() +
  labs(
    y = "number of sequencing reads",
    x = "sample ID",
    title = "ASV reads - controls") + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.text = element_text(size = 8),
    legend.key.size = unit(0.3, "cm"),
    legend.position = "none",
    legend.title = element_blank()
  )
```

what are the main asv's showing up in field blanks? going to group by survey
```{r}
asv_table_filter1 %>%
  filter(sample_type == "field_blank") %>% 
  filter(project == "NBS") %>%
  filter(collection_year == "2022") %>%
  group_by(ASV) %>%
  summarize(mean = mean(reads),
                SD=sd(reads),
                q.025 = quantile(reads,probs=0.025),
                Median = quantile(reads,probs=0.5),
                q.975 = quantile(reads,probs=0.975)) %>%
  arrange(desc(mean))
```


- SBS2022 has some contamination of asv1 (pollock); asv20 (Myoxocephalus sp. (sculpins)); ASV7 (pollock); etc... 
- NBS2021 has lots of contamination - mostly asv1 (pollock); asv3 (chinook); asv2 (herring);  asv11 (sockeye); etc... 
- NBS2022 has some contamination of asv3 (chinook); asv1 (pollock); asv2 (herring); asv40 (cow)
 

okay need to also figure out how to handle this... moving on for now. 

## 3. Discard PCR replicates with low numbers of reads 

calculate reads per sample
```{r}
all_reads <- asv_table_filter1 %>%
  group_by(Sample_ID) %>%
  summarize(ReadsPerSample = sum(reads))
```

visualize 
```{r}
all_reads$x_reordered <- reorder(all_reads$Sample_ID, -all_reads$ReadsPerSample)

all_reads %>%
  ggplot(aes(x = x_reordered, y = ReadsPerSample)) + 
  geom_bar(stat = "identity")
```

fit a normal distribution
```{r}
fit <- MASS::fitdistr(all_reads$ReadsPerSample, "normal")

all_reads %>%  
  mutate(prob = pnorm(all_reads$ReadsPerSample, fit$estimate[[1]], fit$estimate[[2]])) -> all_reads
```

identify and remove the outliers
```{r}
#low_dist_probability_cutoff <- 0.05
minimum_read_cutoff <- 1000

outliers <- all_reads %>% 
  filter(prob < low_dist_probability_cutoff | ReadsPerSample < minimum_read_cutoff)
  #filter(ReadsPerSample < minimum_read_cutoff)
  #filter(prob < low_dist_probability_cutoff)

outlierIDs <- outliers$Sample_ID
```

which samples are removed because of the 1000 reads threshold??
```{r}
replicates_removed <- asv_table_filter1 %>%
  filter(Sample_ID %in% outlierIDs) %>%
  pivot_wider(names_from = "ASV", values_from = "reads")
```

number of pcr replicates removed
```{r}
nrow(replicates_removed)
```

plot them
```{r}
# make a variable for the first and last ASV column in the table
asv_first <- which(colnames(replicates_removed) == "ASV_0001")
asv_last <- ncol(replicates_removed )

replicates_removed %>%
  pivot_longer(cols = asv_first:asv_last, names_to = "ASV", values_to = "count") %>%
ggplot(aes(x=Sample_ID, y=count, fill=ASV)) +
  geom_bar(stat = "identity") + 
    theme_bw() + 
   labs(
    y = "sequencing reads",
    x = "sample ID",
    title = "samples with low read numbers")  +
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.position = "none",
    legend.title = element_blank()
  )  
```

filter the data frame 
```{r}
asv_table_filter2 <- asv_table_filter1 %>%
  filter(!Sample_ID %in% outlierIDs)
```

any ASV's with no reads? 
```{r}
asv_no_reads <- asv_table_filter2 %>%
  group_by(ASV) %>%
  summarise(total = sum(reads)) %>%
  arrange(total)
asv_no_reads
```

# 4. Hierarchical Occupancy Modeling 

this is based on work by Ryan Kelly: https://github.com/invertdna/OccupancyModeling_Stan/tree/master

the hierarchical stan model used here: https://github.com/zjgold/gruinard_decon/blob/master/gruinard_decontam_script.R

```{r}
##Stan Model
sink("Stan_SOM_hierarchical_with_occuprob.stan")
cat(
  "data{/////////////////////////////////////////////////////////////////////
    int<lower=1> S;    // number of samples (nrow)
    int<lower=1> Species[S];    // index of species, each of which will have a different value for p11 and p10
    int<lower=1> Nspecies;    // number of species, each of which will have a different value for p11 and p10
    int<lower=1> L[S];   // index of locations or species/site combinations, each of which will have a different value psi
    int<lower=1> Nloc;   // number of locations or species/site combinations, each of which will have a different value psi
    int<lower=1> K[S];   // number of replicates per site (ncol)
    int<lower=0> N[S]; // number of detections among these replicates
    int z[S];   // integer flag to help estimate psi parameter
}

parameters{/////////////////////////////////////////////////////////////////////
    real<lower=0,upper=1> psi[Nloc];  //commonness parameter
    real<lower=0,upper=1> p11[Nspecies]; //true positive detection rate
    real<lower=0,upper=1> p10[Nspecies]; //false positive detection rate
}

transformed parameters{/////////////////////////////////////////////////////////////////////
}

model{/////////////////////////////////////////////////////////////////////
  real p[S];
  
    for (i in 1:S){
			z[i] ~ bernoulli(psi[L[i]]);
			p[i] = z[i]*p11[Species[i]] + (1-z[i])*p10[Species[i]];
			N[i] ~ binomial(K[i], p[i]);
	}; 
  
  //priors
  psi ~ beta(2,2); 
  p11 ~ beta(2,2); 
  p10 ~ beta(1,10);
}

generated quantities{
  real<lower=0,upper=1> Occupancy_prob[S];    //after inferring parameters above, now calculate occupancy probability for each observation. Equation from Lahoz-Monfort et al. 2015
  
  for (i in 1:S){
  Occupancy_prob[i]  = (psi[L[i]]*(p11[Species[i]]^N[i])*(1-p11[Species[i]])^(K[i]-N[i])) 
  / ((psi[L[i]]*(p11[Species[i]]^N[i])*(1-p11[Species[i]])^(K[i]-N[i])) 
  + (((1-psi[L[i]])*(p10[Species[i]]^N[i]))*((1-p10[Species[i]])^(K[i]-N[i])))
  );
  }
 }
  
",
fill=TRUE)
sink()
```


the first step is to format my data for the stan model - i need site information so will start by getting that from the metadata

note: for NBS/SBS site is treated as the station ID - remove/relabel control samples??? 
```{r}
sites <- metadata %>%
  dplyr::select(Sample_ID, extraction_ID, pcr_replicate, project, collection_year, location1, depth) %>%
  unite(site, project, collection_year, location1, depth, sep = "_", remove = F)

rep_table <- asv_table_filter2 %>%
  left_join(sites, by = "Sample_ID") %>%
  filter(site != 'NA') %>%            ##remove sample for which we don't have site locations
  arrange(site)

occu_df <- rep_table %>%
  ungroup() %>%  
  mutate(reads = ifelse(reads > 0, 1, 0)) %>% # change counts to presence/absence
  dplyr::select(ASV, site, extraction_ID, Sample_ID, reads) %>%
  group_by(ASV, site, extraction_ID) %>%
  summarise(K = n(),  #count the number of rows for K 
            N = sum(reads)) %>% #sum the detections (now in reads column) for N 
  dplyr::rename(Site = site,
         BiologicalRep = extraction_ID) %>% ## just renaming so that it matches the naming used in the stan model
  separate(ASV, into = c(NA, "Species"), sep = 3, remove = FALSE) %>%
  mutate(Species = gsub("_", "", Species))

occu_df$Species <- as.integer(occu_df$Species) #convert ASV to an integer

occu_df <- occu_df %>%
  arrange(Species)
```

**NOTE: i set the extraction_ID as the biological replicate!!!** 

running the occupancy model on this entire data set will take a VERY long time, so now I will reduce the data set down to the patterns of presence (many ASVs/species have identical patterns of presence, aka. pattern of technical reps)

```{r}
pattern.of.replication <- rep_table %>%
  ungroup() %>%  
  mutate(reads = ifelse(reads > 0, 1, 0)) %>% # change counts to presence/absence
 # filter(!grepl("-2-", Sample_ID)) %>%    ## the extraction replicates are messing thing up right now... i need to code this better in the metadata/etc eventually 
  #dplyr::select(location1, biological_replicate, pcr_replicate, ASV, reads) %>%
  dplyr::select(site, extraction_ID, pcr_replicate, ASV, reads) %>%
  pivot_wider(names_from = pcr_replicate, values_from = reads) %>%
  mutate(ndetections = A + B + C) %>%
  group_by(site, ndetections, ASV) %>%
  summarize(tot = sum(!is.na(ndetections)))

pattern.of.presense <- pattern.of.replication %>%
  spread(ndetections, tot, fill = 0) %>%
  unite(repetition.level, '0', '1', '2', '3', sep = '.') %>%
  select(!`<NA>`)

#select a representative 
unique.pattern <- pattern.of.presense %>%
  group_by(repetition.level) %>%
  summarise(ASV = head(ASV,1) ,
            Site = head(site, 1)) %>%
  unite(Site, ASV, col = 'key', sep = '.', remove = F)

#subset my full data frame (occu_df) to just include one representative of each unique detection pattern 
occu_df_subset <- occu_df %>%
  unite(Site, ASV, col = 'key', sep = '.', remove = F) %>%
  filter(key %in% unique.pattern$key) 
```

0.0.0.3 = three detections in three reps
2.0.1.0 = zero detections in two reps and two detections in one reps
1.0.0.0 = zero detection in one rep 

```{r}
temp.df <- occu_df_subset

  #make a new species column to that values are consecutive
  Species <- temp.df$Species
  temp.df$Species_1 <- match(Species, unique(Species))

  #create unique identifier for combinations of site-biologicalrep-ASV; for use in hierarchical modeling
  SDS <- unite(data = temp.df, col = SDS, c("Site", "BiologicalRep", "Species")) %>% pull(SDS)
  temp.df$SiteRepSpecies <- match(SDS, unique(SDS)) #index for unique site-biologicalrep-species combinations
  
  #create unique identifier for combinations of site-ASV; for use in hierarchical modeling
  SS <- unite(data = temp.df, col = SS, c("Site", "Species")) %>% pull(SS)
  temp.df$SiteSpecies <- match(SS, unique(SS)) #index for unique site-species combinations
  
  #####################
  #run Stan model
  #note this will take a while the first time you run a particular model, because it needs to compile from C++
  #####################      
  myHierarchicalModel <- stan(file = "Stan_SOM_hierarchical_with_occuprob.stan", 
                        data = list(
                          S = nrow(temp.df),
                          Species = temp.df$Species_1,
                          Nspecies = length(unique(temp.df$Species_1)),
                          L = temp.df$SiteSpecies,
                          Nloc = length(unique(temp.df$SiteSpecies)),
                          K = temp.df$K,
                          N = temp.df$N,
                          z = ifelse(temp.df$N > 0, 1, 0)
                             ), 
                             chains = 4,   #number of chains
                             iter = 4000   #number of iterations per chain
       )
       
  myHierarchicalStanResults <- tidy(tibble(as.data.frame(myHierarchicalModel)))
  
  #write_rds(myHierarchicalStanResults, "~/BeringSea_Arctic_eDNA/data/NBS_SBS_2022_occupancy_output_20240429.rds")
```


```{r}
myHierarchicalStanResults <- read_rds("~/BeringSea_Arctic_eDNA/data/NBS_SBS_2022_occupancy_output_20240429.rds")

  ## occupancy probabilities 
  myHierarchicalStanResults_occu <- myHierarchicalStanResults %>%
    filter(grepl("Occupancy_prob", column)) %>%
    separate(column, into=c("column","SiteRepSpecies"), sep="([\\[\\]])")
  
  myHierarchicalStanResults_occu$SiteRepSpecies <- as.numeric(myHierarchicalStanResults_occu$SiteRepSpecies)
  
  occupancy_prob <- temp.df %>% 
    select(ASV, Species, Site, SiteSpecies, SiteRepSpecies) %>%
    left_join(myHierarchicalStanResults_occu, by = "SiteRepSpecies") %>% 
    group_by(ASV, Site, SiteSpecies) %>%
    summarise(max_Occupancy_prob = max(mean))
  
# join my occupancy probabilities the unique.pattern df and then the pattern of presence... 
occu_with_key <- occupancy_prob %>%
  unite(Site, ASV, col = 'key', sep = '.', remove = F) %>%
  left_join(unique.pattern, by = c('key', 'ASV', 'Site')) 

occu_with_key_to_join <- occu_with_key[, c("repetition.level", "max_Occupancy_prob")]

site.asv_occupancy_probs <- pattern.of.presense %>%
  left_join(occu_with_key_to_join)

keepers <- site.asv_occupancy_probs %>%
  filter(max_Occupancy_prob >= 0.8) %>%
  unite(site, ASV, col = 'filter_id', sep = '.', remove = F)
discard <- site.asv_occupancy_probs %>%
  filter(max_Occupancy_prob < 0.8)
```

double check that occupancy threshold is appropriate. since occu 0.79 was all 1.0.0.0 (no detections in one rep), the 0.8 cutoff is good.  

```{r}
asv_table_filter3 <- rep_table %>% 
  unite(site, ASV, col = 'loc.asv', sep = '.', remove = F) %>%
  filter(loc.asv %in% keepers$filter_id)
```

any ASV's with no reads? 
```{r}
asvs_noreads <- asv_table_filter3 %>%
  group_by(ASV) %>%
  summarise(total = sum(reads)) %>%
  filter(total == 0)
```

if so, remove them
```{r}
asv_table_filter3 <- asv_table_filter3 %>% 
  filter(!ASV %in% asvs_noreads$ASV)
```

how many ASVs are left? 
```{r}
length(unique(asv_table_filter3$ASV)) 
length(unique(asv_table_filter3$Sample_ID)) 
```
 
# 5. Dissimilarity between PCR (biological) replicates 

This step removes samples for which the dissimilarity between PCR replicates exceeds the normal distribution of dissimilarities observed in samples. The objective of this step is to remove any technical replicates that look like they do not belong.

are there any samples that have made it to this point that don't actually have any reads? 
```{r}
ZeroReadCount <- asv_table_filter3 %>%
  group_by(Sample_ID) %>%
  summarise(total_reads = sum(reads)) %>%
  filter(total_reads == 0)
ZeroReadCount 
```

```{r}
asv_table_filter3 <- asv_table_filter3 %>% 
  filter(!Sample_ID %in% ZeroReadCount$Sample_ID)
```

how many pcr replicates does each extraction replicate have? 
```{r}
OneRep <- asv_table_filter3 %>%
  group_by(extraction_ID) %>%
  summarise(nrep = n_distinct(Sample_ID)) %>%
  #filter(nrep == 2)  # there are ten
  filter(nrep == 1) # there are six
OneRep
```

can't run this test on extraction with only one rep
```{r}
temp_table <- asv_table_filter3 %>%
  filter(!extraction_ID %in% OneRep$extraction_ID)
```
good to go. 

first, i'll calculate an eDNA index --- CHECK THIS! 
```{r}
normalized <- temp_table %>%
  group_by(Sample_ID) %>%
  mutate(Tot = sum(reads),
         Prop_reads = reads/Tot) %>%
  dplyr::group_by(ASV) %>%
  mutate(Colmax = max(Prop_reads, na.rm = TRUE),
         Normalized_reads = Prop_reads/Colmax)

#add a new sample id column that also includes the location - will use this for dissimilarity measures
normalized <- normalized %>%
  unite(site_biorep, site, extraction_ID, sep = ".", remove = FALSE) %>%
  unite(new_ID, site_biorep, pcr_replicate, sep = "-", remove = FALSE)
```


```{r}
tibble_to_matrix <- function (tb) {
  
  tb %>%
  #normalized %>%
    group_by(new_ID, ASV) %>% 
    summarise(nReads = sum(Normalized_reads)) %>% 
    spread ( key = "ASV", value = "nReads", fill = 0) %>%
    ungroup() -> matrix_1
    samples <- pull (matrix_1, new_ID)
    matrix_1[,-1] -> matrix_1
    data.matrix(matrix_1) -> matrix_1
    dimnames(matrix_1)[[1]] <- samples
    vegdist(matrix_1) -> matrix_1
}

```


```{r}
all.distances.full <- tibble_to_matrix(normalized)

# Do all samples have a name?
summary(is.na(names(all.distances.full)))
```

make the pairwise distances a long table  --- NEED TO FIX THIS !!!! 
```{r}
as_tibble(subset(melt(as.matrix(all.distances.full)))) -> all.distances.melted

# Any major screw ups
summary(is.na(all.distances.melted$value))

# Now, create a three variables for all distances, they could be PCR replicates, BIOL replicates, or from the same site

all.distances.melted %>%
  separate (X1, into = "Bottle1", sep = "\\-", remove = FALSE) %>%
  separate (Bottle1, into = "Site1", sep = "\\.", remove = FALSE) %>%
  separate (X2, into ="Bottle2", sep = "\\-", remove = FALSE) %>%
  separate (Bottle2, into = "Site2", sep = "\\.",remove = FALSE) %>%
  mutate ( #Day.site1 = str_sub(Bottle1, start = 1, end = -2),
           #Day.site2 = str_sub(Bottle2, start = 1, end = -2),
           Distance.type = case_when( Bottle1 == Bottle2 ~ "PCR.replicates",
                                      #Day.site1 == Day.site2 ~ "Biol.replicates",
                                      Site1 == Site2 ~ "Same Site",
                                      TRUE ~ "Different Site"
                                     )) %>%
  dplyr::select(Sample1 = X1, Sample2 = X2 , value , Distance.type) %>%
  filter (Sample1 != Sample2) -> all.distances.to.plot

# Checking all went well
sapply(all.distances.to.plot, function(x) summary(is.na(x)))
```

```{r}
all.distances.to.plot$Distance.type <- all.distances.to.plot$Distance.type  %>% fct_relevel("PCR.replicates", "Same Site")

ggplot (all.distances.to.plot) +
  geom_histogram (aes (fill = Distance.type, x = value, after_stat(ndensity)), position = "dodge",  alpha = 0.9, bins = 50) +
  facet_wrap( ~ Distance.type) +
  labs (x = "Pairwise dissimilarity", y = "density" ,
        Distance.type = "Distance") +
    guides (fill = "none")

  #ggsave("visual.anova.png", dpi = "retina")
```

there is greater dissimiarity for PCR replicates and same site samples than i'd ideally like to be seeing... is this trend specific to a dataset?? 

```{r}
all.distances.to.plot %>%
  separate(Sample1, into = c("project", "collection_year", "location1", "depth", "extraction_ID", "pcr_replicate")) %>%
  filter(project == "NBS") %>%
  filter(collection_year == 2021) %>%
  ggplot() +
  geom_histogram (aes (fill = Distance.type, x = value, after_stat(ndensity)), position = "dodge",  alpha = 0.9, bins = 50) +
  facet_wrap( ~ Distance.type) +
  labs (x = "Pairwise dissimilarity", y = "density" ,
        Distance.type = "Distance") +
    guides (fill = "none")
```

dissimilarity a bit high for pcr reps and samples from same site here... 

```{r}
all.distances.to.plot %>%
  separate(Sample1, into = c("project", "collection_year", "location1", "depth", "extraction_ID", "pcr_replicate")) %>%
  filter(project == "NBS") %>%
  filter(collection_year == 2022) %>%
  ggplot() +
  geom_histogram (aes (fill = Distance.type, x = value, after_stat(ndensity)), position = "dodge",  alpha = 0.9, bins = 50) +
  facet_wrap( ~ Distance.type) +
  labs (x = "Pairwise dissimilarity", y = "density" ,
        Distance.type = "Distance") +
    guides (fill = "none")
```

this looks good. 

```{r}
all.distances.to.plot %>%
  separate(Sample1, into = c("project", "collection_year", "location1", "depth", "extraction_ID", "pcr_replicate")) %>%
  filter(project == "SBS") %>%
  filter(collection_year == 2022) %>%
  ggplot() +
  geom_histogram (aes (fill = Distance.type, x = value, after_stat(ndensity)), position = "dodge",  alpha = 0.9, bins = 50) +
  facet_wrap( ~ Distance.type) +
  labs (x = "Pairwise dissimilarity", y = "density" ,
        Distance.type = "Distance") +
    guides (fill = "none")
```

fairly low sample number for SBS2022 right now - still need to sequence the rest of the samples and then check what's going on 

next i will follow what was done here:  (https://github.com/ramongallego/eDNA.and.Ocean.Acidification.Gallego.et.al.2020/blob/master/Scripts/Denoising.all.runs.Rmd) and instead of choosing outliers based on the pairwise distances, we can do a similar thing using the distance to centroid. 


now identify and discard outliers 
```{r message=FALSE, warning=FALSE}
normalized %>%
  group_by(extraction_ID) %>% nest() -> nested.cleaning 

nested.cleaning %>% 
  mutate(matrix = map(data, tibble_to_matrix)) -> nested.cleaning

nested.cleaning %>% mutate(ncomparisons = map(matrix, length)) -> nested.cleaning
```

```{r}
dist_to_centroid <- function (x,y) {
  
  #biol <- rep(y, dim(x)[[1]])
  biol <- rep(y, length(x))
  
  if (length(biol) == 1) {
    output = rep(x[1]/2,2)
    names(output) <- attr(x, "Labels")
  }else{ 
    
  dispersion <- betadisper(x, group = biol)
  output = dispersion$distances
  }
  output
    }
```

```{r}
nested.cleaning.temp <- nested.cleaning %>% 
  mutate(distances = map2(matrix, extraction_ID, dist_to_centroid))

all_distances <- nested.cleaning.temp %>%
  unnest_longer(distances) %>%
  dplyr::select(extraction_ID, distances_id, distances)

hist(all_distances$distances)
```

calculate normal distribution of distances to centroid
```{r}
normparams <- MASS::fitdistr(all_distances$distances, "normal")$estimate                                      
probs <- pnorm(all_distances$distances, normparams[1], normparams[2])
outliers_centroid <- which(probs>0.95)

discard_centroid <- all_distances$distances_id[outliers_centroid]
discard_centroid
```

all of these are from NBS2021 or SBS2022. 

output the samples that pass this filter
```{r}
asv_table_filter4 <- temp_table %>%
  unite(site_biorep, site, extraction_ID, sep = ".", remove = FALSE) %>%
  unite(new_ID, site_biorep, pcr_replicate, sep = "-", remove = FALSE) %>%
  filter(!new_ID %in% discard_centroid)
```


which extraction IDs were removed??
```{r}
to_discard <- data.frame(discard_centroid) %>%
  separate(discard_centroid, into = c("project", "collection_year", "location1", "depth", "extraction_ID", "pcr_replicate")) %>%
  unite(site, project, collection_year, location1, depth, sep = "_", remove = F)

removed_rep_for_dissimilarity <- temp_table %>%
  filter(extraction_ID %in% to_discard$extraction_ID)
```

these samples have at least one dissimilar pcr replicates 
```{r}
unique(removed_rep_for_dissimilarity$extraction_ID)

length(unique(removed_rep_for_dissimilarity$extraction_ID))  ## these are just the extraction_ID with at least one PCR rep removed 

first_six <- unique(removed_rep_for_dissimilarity$extraction_ID)[1:6]

removed_rep_for_dissimilarity %>%
  filter(extraction_ID %in% first_six) %>%
  group_by(Sample_ID) %>%
  mutate(sum=sum(reads)) %>%
  mutate(prop = reads/sum) %>%
  ggplot(aes(x=Sample_ID, y=prop, fill=ASV)) +
  geom_bar(stat = "identity") + 
  facet_wrap(~extraction_ID, scales = 'free', ncol = 3) +
  theme_bw() + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.position = "none",
    legend.title = element_blank()
  )  
```


will need to think more about how to filter samples based on dissimilarity, there is more dissimilarity across pcr replicates in this dataset than i've encountered before... 


# Some results of read decontamination  

plot all field samples
```{r}
asv_table_filter4 %>%
  filter(sample_type == "sample") %>%
  ggplot(aes(x=Sample_ID, y=reads, fill=ASV)) +
  geom_bar(stat = "identity") + 
  facet_grid(project.x~collection_year.x, scales = "free_x") + 
  theme_bw() +
  labs(
    y = "number of sequencing reads",
    x = "sample ID",
    title = "ASV reads - field samples") + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.text = element_text(size = 8),
    legend.key.size = unit(0.3, "cm"),
    legend.position = "none",
    legend.title = element_blank()
  )
```

```{r}
asv_table_filter4 %>%
  filter(sample_type == "sample") %>%
  group_by(Sample_ID, project.x, collection_year.x) %>%
  summarize(total_reads = sum(reads)) %>%
  ungroup() %>%
  group_by(project.x, collection_year.x) %>%
    summarize(total_reads_mean = mean(total_reads),
            total_reads_sd = sd(total_reads),
            ci_lower = total_reads_mean - qt(0.975, df = n() - 1) * (total_reads_sd / sqrt(n())),
            ci_upper = total_reads_mean + qt(0.975, df = n() - 1) * (total_reads_sd / sqrt(n())))
```



**NBS 2021 station 11 at 10m**
```{r}
asv_table_filter4 %>%
  group_by(Sample_ID) %>%
  mutate(sum=sum(reads)) %>%
  mutate(prop = reads/sum) %>%
  filter(site == "NBS_2021_11_10") %>%
  ggplot(aes(x=Sample_ID, y=prop, fill=ASV)) +
  geom_bar(stat = "identity") + 
  facet_wrap(~extraction_ID, scales = 'free', ncol = 3) +
  theme_bw() + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.position = "none",
    legend.title = element_blank()
  )  
```

**NBS 2022 station 5 at 0m**
```{r}
asv_table_filter4 %>%
  group_by(Sample_ID) %>%
  mutate(sum=sum(reads)) %>%
  mutate(prop = reads/sum) %>%
  filter(site == "NBS_2022_5_0") %>%
  ggplot(aes(x=Sample_ID, y=prop, fill=ASV)) +
  geom_bar(stat = "identity") + 
  facet_wrap(~extraction_ID, scales = 'free', ncol = 3) +
  theme_bw() + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.position = "none",
    legend.title = element_blank()
  )  
```

**SBS 2022 station 24 at bottom**
```{r}
asv_table_filter4 %>%
  group_by(Sample_ID) %>%
  mutate(sum=sum(reads)) %>%
  mutate(prop = reads/sum) %>%
  filter(site == "SBS_2022_24_bottom") %>%
  ggplot(aes(x=Sample_ID, y=prop, fill=ASV)) +
  geom_bar(stat = "identity") + 
  facet_wrap(~extraction_ID, scales = 'free', ncol = 3) +
  theme_bw() + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.position = "none",
    legend.title = element_blank()
  )  
```

now what do the field blanks look like???
```{r}
asv_table_filter4 %>%
  filter(sample_type == "field_blank") %>%
  filter(collection_year.x == "2021") %>%
  filter(project.x == "NBS") %>%
  group_by(Sample_ID) %>%
  mutate(sum=sum(reads)) %>%
  mutate(prop = reads/sum) %>%
  ggplot(aes(x=Sample_ID, y=prop, fill=ASV)) +
  geom_bar(stat = "identity") + 
  facet_wrap(~extraction_ID, scales = 'free', ncol = 3) +
  theme_bw() + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.position = "none",
    legend.title = element_blank()
  )  
```

and for SBS
```{r}
asv_table_filter4 %>%
  filter(sample_type == "field_blank") %>%
  filter(collection_year.x == "2022") %>%
  filter(project.x == "SBS") %>%
  group_by(Sample_ID) %>%
  mutate(sum=sum(reads)) %>%
  mutate(prop = reads/sum) %>%
  ggplot(aes(x=Sample_ID, y=prop, fill=ASV)) +
  geom_bar(stat = "identity") + 
  facet_wrap(~extraction_ID, scales = 'free', ncol = 3) +
  theme_bw() + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.position = "none",
    legend.title = element_blank()
  )  
```

done for now, but this data set will require more thoughtful clean-up. 

```{r}
#write.csv(asv_table_filter4, "/home/kimberly.ledger/BeringSea_Arctic_eDNA/data/asv_table.csv")
```

