---
title: "checking dadasnake outputs"
author: "Kimberly Ledger"
date: "2024-06-17"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(root.dir = "/home/kimberly.ledger/BeringSea_Arctic_eDNA/")
```

libraries
```{r}
library(tidyverse)
rename <- dplyr::rename
```

load table with dada2 reporting - this table includes the number of reads for each samples thru the filtering steps
```{r}
report <- read.delim("/home/kimberly.ledger/BeringSea_Arctic_eDNA/data/dadasnake/post_finalNumbers_perSample.tsv", header = TRUE, sep = "\t")
```
*note: the subset of SBS samples that were sequenced twice are combined in this output

one sample was mislabeled in the sample sheet to fixing that here
```{r}
report <- report %>%
  mutate(sample = ifelse(sample == "e0683-A", "e00683-A", sample)) %>%
  mutate(sample = ifelse(sample == "e0683-B", "e00683-B", sample)) %>%
  mutate(sample = ifelse(sample == "e0683-C", "e00683-C", sample))        
```

load sample type and other library prep info
```{r}
sample_metadata <- read.csv("/home/kimberly.ledger/BeringSea_Arctic_eDNA/data/sample_names_NBS_SBS_DBO.csv")

#illumina output changed "_" to "-"
sample_metadata$sample_ID <- gsub("_", "-", sample_metadata$sample_ID) 
```

join the report and metadata
```{r}
report_meta <- report %>%
  rename(sample_ID = sample) %>%
  left_join(sample_metadata)
```

take a course look at the report by project, year, sample_type
```{r}
report_table <- report_meta %>%
  group_by(project, collection_year, run, sample_type) %>%
  summarise(mean_reads = mean(reads_tax.length_filtered),
            reads_q.05 = quantile(reads_tax.length_filtered, probs=0.05),
            median_q.5 = median(reads_tax.length_filtered),
            reads_q.95 = quantile(reads_tax.length_filtered, probs=0.95))

report_table
```

make a quick visual of this 
```{r}
report_table %>%
  unite(col = "project_year", project, collection_year, sep = "_", remove = F) %>%
  ggplot(aes(x=sample_type, y=median_q.5, fill=sample_type)) +
  geom_bar(stat = "identity") +
  facet_grid(~project_year, scales = "free_x") + 
  theme_bw() +
  labs(
    y = "median # of sequencing reads",
    x = "sample type") + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.text = element_text(size = 8),
    legend.key.size = unit(0.3, "cm"),
    legend.position = "none",
    legend.title = element_blank()
  )
```

from this i need to:
1) see what went awry with the pcr blanks with DBO23 libraries
2) look into field blanks of DBO23 libraries
3) i already knew that NBS21 field blanks have plenty of contamination
4) compare the SBS22 samples including in two different library preps



### starting with a closer look into DBO samples

before getting concerned about sequences in blanks, let's check if there are fish sequences or not by blasting ASVs for taxonomic identification 

check sequence table outputs
```{r}
asv_table <- readRDS("/home/kimberly.ledger/BeringSea_Arctic_eDNA/data/dadasnake/filtered.seqTab.RDS") %>%
  select(!Row.names)

#transpose 
asv_table <- data.frame(t(asv_table))

#set column names to be ASV# 
colnames(asv_table) <- asv_table["ASV",]

#remove row that has ASV#
asv_table <- asv_table[!rownames(asv_table) %in% c('ASV'), ]

#make sure reads are numbers
# Convert all character columns to numeric
for (col in names(asv_table)) {
  asv_table[[col]] <- as.numeric(asv_table[[col]])
}

#make make sample ID a column 
asv_table$sample_ID <- rownames(asv_table)

#rename the one sample that got the wrong ID in the sample sheet 
asv_table <- asv_table %>%
  mutate(sample_ID = ifelse(sample_ID == "e0683-A", "e00683-A", sample_ID)) %>%
  mutate(sample_ID = ifelse(sample_ID == "e0683-B", "e00683-B", sample_ID)) %>%
  mutate(sample_ID = ifelse(sample_ID == "e0683-C", "e00683-C", sample_ID))
```

add column to the ASV table that labels the sample type
```{r}
asv_table_with_sample_type <- sample_metadata %>%
  dplyr::select(sample_ID, sample_type, collection_year, project) %>%
  left_join(asv_table, by = "sample_ID") %>%
   unite(col = "project_year", project, collection_year, sep = "_", remove = F)

# make a variable for the first and last ASV column in the table
asv_first <- which(colnames(asv_table_with_sample_type) == "ASV_0001")
asv_last <- ncol(asv_table_with_sample_type)
```


let me look into the reads that got into the pcr blanks
```{r}
asv_table_with_sample_type %>%
  pivot_longer(cols = c(asv_first:asv_last), names_to = "ASV", values_to = "reads") %>%
  filter(sample_type == "pcr_blank") %>%
  ggplot(aes(x=sample_ID, y=reads, fill=ASV)) +
  geom_bar(stat = "identity") + 
  theme_bw() +
  facet_grid(~project_year, scales = "free_x") + 
  labs(
    y = "number of sequencing reads",
    x = "sample ID",
    title = "ASV reads - pcr blanks") + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.text = element_text(size = 8),
    legend.key.size = unit(0.3, "cm"),
    legend.position = "none",
    legend.title = element_blank()
  )
```


```{r}
asvs_PCRN <- asv_table_with_sample_type %>%
  pivot_longer(cols = c(asv_first:asv_last), names_to = "ASV", values_to = "reads") %>%
  filter(sample_type == "pcr_blank") %>%
  group_by(project_year, ASV) %>%
  summarise(total = sum(reads)) %>%
  arrange(desc(total))

head(asvs_PCRN, 10)
```

asv_0014 is the sturgeon PC so perhaps i just switched around the NC and PC in that plate... not a problem.
asv_0013 is salmon. 


let me look into the reads that got into the field blanks
```{r}
asv_table_with_sample_type %>%
  pivot_longer(cols = c(asv_first:asv_last), names_to = "ASV", values_to = "reads") %>%
  filter(sample_type == "field_blank") %>%
  ggplot(aes(x=sample_ID, y=reads, fill=ASV)) +
  geom_bar(stat = "identity") + 
  theme_bw() +
  facet_grid(~project_year, scales = "free_x") + 
  labs(
    y = "number of sequencing reads",
    x = "sample ID",
    title = "ASV reads - field blanks") + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.text = element_text(size = 8),
    legend.key.size = unit(0.3, "cm"),
    legend.position = "none",
    legend.title = element_blank()
  )
```

```{r}
asvs_FN <- asv_table_with_sample_type %>%
  filter(project == "SBS") %>%
  pivot_longer(cols = c(asv_first:asv_last), names_to = "ASV", values_to = "reads") %>%
  filter(sample_type == "field_blank") %>%
  group_by(project_year, ASV) %>%
  summarise(total = sum(reads)) %>%
  arrange(desc(total))

head(asvs_FN, 10)
```

DBO23 problems = asv_0057 = pig; asv_0017 = salmon; asv_0030 = hexagrammos; asv_0107 = salvelinus
NBS21 problems = asv_0001 = pollock; asv_0008 = chinook salmon; asv_0002 = herring
NBS22 problems = 
NBS23 problems = pollock; gadus
SBS22 problems = avs_0022 = Phasianidae (chicken?); chinook salmon, pollock 


## now let's compare the SBS22 samples that were duplicated 

since the filter.seqTab already combined sequences for a sample from the two runs, i need to look back at the seqTab for seperate runs

first, get the sample_ID for those of interest 

```{r}
sbs_p16 <- sample_metadata %>%
  filter(project == "SBS") %>%
  filter(extraction_plate == "16_2023")
```

first sequencing run
```{r}
seq1 <- readRDS("/home/kimberly.ledger/BeringSea_Arctic_eDNA/data/dadasnake/filtered.seqTab.RDS") %>%
  select(!Row.names)

#transpose 
seq1 <- data.frame(t(seq1))

#set column names to be ASV# 
colnames(seq1) <- seq1["ASV",]

#remove row that has ASV#
seq1 <- seq1[!rownames(seq1) %in% c('ASV'), ]

#make sure reads are numbers
# Convert all character columns to numeric
for (col in names(seq1)) {
  seq1[[col]] <- as.numeric(seq1[[col]])
}

#make make sample ID a column 
seq1$sample_ID <- rownames(seq1)

seq1_sbs16 <- seq1 %>%
  filter(sample_ID %in% sbs_p16$sample_ID)
```

third sequencing run
```{r}
seq3 <- readRDS("/home/kimberly.ledger/BeringSea_Arctic_eDNA/data/dadasnake/filtered.seqTab.RDS") %>%
  select(!Row.names)

#transpose 
seq3 <- data.frame(t(seq3))

#set column names to be ASV# 
colnames(seq3) <- seq3["ASV",]

#remove row that has ASV#
seq3 <- seq3[!rownames(seq3) %in% c('ASV'), ]

#make sure reads are numbers
# Convert all character columns to numeric
for (col in names(seq3)) {
  seq3[[col]] <- as.numeric(seq3[[col]])
}

#make make sample ID a column 
seq3$sample_ID <- rownames(seq3)

seq3_sbs16 <- seq3 %>%
  filter(sample_ID %in% sbs_p16$sample_ID)
```

ahhh. nevermind!  this sequence counts in seqTab.1, etc are all the same... not split up by run. might need to reanalyze the data???



okay, i will want to clean up the ASV table a bit before going hard on the taxonomic assignment step. right now there are 1548 ASVs. most are likely junk? 

## Account for contaminants in positive and negative controls 

next we will remove ASVs that only occur in controls and not in environmental samples. 

number of reads
```{r}
reads_per_type_ASV <- asv_table_with_sample_type %>%
  pivot_longer(cols = c(asv_first:asv_last), names_to = "ASV", values_to = "reads") %>%
  group_by(ASV, sample_type) %>%
  summarize(TotalReadsPerASV = sum(reads, na.rm = TRUE)) %>%
  arrange(ASV)
```

what ASVs have no reads in samples, but reads in the controls? 
```{r}
not_in_samples <- reads_per_type_ASV %>%
  pivot_wider(names_from = "sample_type", values_from = c("TotalReadsPerASV")) %>%
    filter(sample < 1)
head(not_in_samples)
```


what ASVs have reads in samples, but more reads in the controls? 
```{r}
more_in_pcr_blanks <- reads_per_type_ASV %>%
  pivot_wider(names_from = "sample_type", values_from = c("TotalReadsPerASV")) %>%
  filter(sample > 1) %>%
  filter(pcr_blank > sample)
head(more_in_pcr_blanks)

more_in_extraction_blanks <- reads_per_type_ASV %>%
  pivot_wider(names_from = "sample_type", values_from = c("TotalReadsPerASV")) %>%
  filter(sample > 1) %>%
  filter(extraction_blank > sample)
head(more_in_extraction_blanks)

more_in_pc_blanks <- reads_per_type_ASV %>%
  pivot_wider(names_from = "sample_type", values_from = c("TotalReadsPerASV")) %>%
  filter(sample > 1) %>%
  filter(positive > sample)
head(more_in_pc_blanks)

more_in_fb_blanks <- reads_per_type_ASV %>%
  pivot_wider(names_from = "sample_type", values_from = c("TotalReadsPerASV")) %>%
  filter(sample > 1) %>%
  filter(field_blank > sample)
head(more_in_fb_blanks)
```


remove these from the asv table - for now not getting rid of positive control asv (asv_0014)
```{r}
asv_table_filter1 <- asv_table_with_sample_type %>%
  pivot_longer(cols = c(asv_first:asv_last), names_to = "ASV", values_to = "reads") %>%
  filter(!ASV %in% not_in_samples$ASV) %>%
  #filter(!ASV %in% more_in_pcr_blanks$ASV) %>%
  filter(!ASV %in% more_in_extraction_blanks$ASV) %>%
  #filter(!ASV %in% more_in_pc_blanks$ASV) %>%
  filter(!ASV %in% more_in_fb_blanks$ASV)
```

let's plot the distribution of reads per ASV
```{r}
reads_per_ASV <- asv_table_filter1 %>%
  filter(!is.na(reads)) %>%
  group_by(ASV) %>%
  summarize(ReadsPerASV = sum(reads))

hist(reads_per_ASV$ReadsPerASV)
hist(log10(reads_per_ASV$ReadsPerASV))
```

okay, maybe not remove ASVs just because they are rare... 


## 3. Discard PCR replicates with low numbers of reads 

calculate reads per sample
```{r}
all_reads <- asv_table_filter1 %>%
  group_by(sample_ID) %>%
  summarize(ReadsPerSample = sum(reads, na.rm = T))
```

visualize 
```{r}
all_reads$x_reordered <- reorder(all_reads$sample_ID, -all_reads$ReadsPerSample)

all_reads %>%
  ggplot(aes(x = x_reordered, y = ReadsPerSample)) + 
  geom_bar(stat = "identity")
```

fit a normal distribution
```{r}
fit <- MASS::fitdistr(all_reads$ReadsPerSample, "normal")

all_reads %>%  
  mutate(prob = pnorm(all_reads$ReadsPerSample, fit$estimate[[1]], fit$estimate[[2]])) -> all_reads
```

identify and remove the outliers
```{r}
#low_dist_probability_cutoff <- 0.05    ### THIS IS NOT DOING WHAT IT SHOULD>>> PROBABLY BECAUSE IT"S NOT A NORMAL DIST
minimum_read_cutoff <- 250

outliers <- all_reads %>% 
  #filter(prob < low_dist_probability_cutoff | ReadsPerSample < minimum_read_cutoff)
  filter(ReadsPerSample < minimum_read_cutoff)
  #filter(prob < low_dist_probability_cutoff)

outlierIDs <- outliers$sample_ID
```

1000 read cut-off = 477
500 read cut-off = 441
250 read cut off = 374 

## 1000 read-cut off is removing about 25% of samples... some okay because they are negatives, but probably this is too stringent
but the cut off will need to be at least 250 reads per sample so i'll go with that for now... 
        
which samples are removed because of the 1000 reads threshold??
```{r}
replicates_removed <- asv_table_filter1 %>%
  filter(sample_ID %in% outlierIDs) %>%
  pivot_wider(names_from = "ASV", values_from = "reads")
```

number of pcr replicates removed
```{r}
nrow(replicates_removed)
```

plot them
```{r}
# make a variable for the first and last ASV column in the table
asv_first <- which(colnames(replicates_removed) == "ASV_0001")
asv_last <- ncol(replicates_removed )

replicates_removed %>%
  pivot_longer(cols = asv_first:asv_last, names_to = "ASV", values_to = "count") %>%
ggplot(aes(x=sample_ID, y=count, fill=ASV)) +
  geom_bar(stat = "identity") + 
    theme_bw() + 
   labs(
    y = "sequencing reads",
    x = "sample ID",
    title = "samples with low read numbers")  +
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.position = "none",
    legend.title = element_blank()
  )  
```

filter the data frame 
```{r}
asv_table_filter2 <- asv_table_filter1 %>%
  filter(!sample_ID %in% outlierIDs)
```

any ASV's with no reads? 
```{r}
asv_no_reads <- asv_table_filter2 %>%
  group_by(ASV) %>%
  summarise(total = sum(reads)) %>%
  filter(total == 0)
asv_no_reads
```


now lets see how many of these ASVs have taxonomic IDs. 
```{r}
taxonomy <- read.csv("/home/kimberly.ledger/BeringSea_Arctic_eDNA/outputs/collapsed_tax_20240625.csv") %>%
  select(!X)
```

```{r}
asv_table_filter3 <- asv_table_filter2 %>%
  filter(!ASV %in% asv_no_reads$ASV) %>%
  left_join(taxonomy)
```

what ASV's do not have a taxonomic ID? 
```{r}
asv_table_filter3 %>%
  filter(is.na(taxon)) %>%
  group_by(ASV) %>%
  summarize(total_reads = sum(reads))
```

okay, so lots of ASV without an ID... 
14 = positive control (sturgeon) 
28 = human
33 = na
50 = na
61 = bos
71 = na
96 = na
98 = Cottidae (maybe?)
etc..... 

okay so this seems reasonable to do 
```{r}
asv_table_filter4 <- asv_table_filter3 %>%
  filter(!is.na(taxon))
```


now let's change from ASV to taxon 
```{r}
taxon_table <- asv_table_filter4 %>%
  group_by(sample_ID, sample_type, project_year, collection_year, project, taxon, taxonomic_level) %>%
  summarise(reads = sum(reads))
```

okay now let's check out the data using taxon IDs 

```{r}
taxon_table %>%
  filter(sample_type == "field_blank") %>%
  ggplot(aes(x=sample_ID, y=reads, fill=taxon)) +
  geom_bar(stat = "identity") + 
  theme_bw() +
  facet_grid(~project_year, scales = "free_x") + 
  labs(
    y = "number of sequencing reads",
    x = "sample ID",
    title = "ASV reads - field blanks") + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.text = element_text(size = 6),
    legend.key.size = unit(0.3, "cm"),
    legend.position = "none",
    legend.title = element_blank()
  )
```

```{r}
taxon_table %>%
  filter(sample_type == "field_blank") %>%
  filter(reads > 0) %>%
  arrange(desc(reads))
```

```{r}
taxon_table %>%
  filter(sample_type == "pcr_blank") %>%
  ggplot(aes(x=sample_ID, y=reads, fill=taxon)) +
  geom_bar(stat = "identity") + 
  theme_bw() +
  facet_grid(~project_year, scales = "free_x") + 
  labs(
    y = "number of sequencing reads",
    x = "sample ID",
    title = "ASV reads - pcr blanks") + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.text = element_text(size = 6),
    legend.key.size = unit(0.3, "cm"),
    legend.position = "none",
    legend.title = element_blank()
  )
```

```{r}
taxon_table %>%
  filter(sample_type == "pcr_blank") %>%
  filter(reads > 0)
```

```{r}
taxon_table %>%
  filter(sample_type == "positive") %>%
  ggplot(aes(x=sample_ID, y=reads, fill=taxon)) +
  geom_bar(stat = "identity") + 
  theme_bw() +
  facet_grid(~project_year, scales = "free_x") + 
  labs(
    y = "number of sequencing reads",
    x = "sample ID",
    title = "ASV reads - positive") + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.text = element_text(size = 6),
    legend.key.size = unit(0.3, "cm"),
    legend.position = "none",
    legend.title = element_blank()
  )
```

okay, some tag-jumping that should be accounted for. 
```{r}
taxon_table %>%
  filter(sample_type == "positive") %>%
  filter(reads > 0)
```

are reads of rare taxa in just one pcr rep or many??? 
```{r}
taxon_totals <- taxon_table %>%
  group_by(taxon) %>%
  summarize(reads = sum(reads)) %>%
  arrange(reads)

taxon_table %>%
  filter(taxon == "Phoca fasciata") %>%
  filter(reads > 0)

taxon_table %>%
  filter(taxon == "Blepsias") %>%
  filter(reads > 0)

taxon_table %>%
  filter(taxon == "Balaenoptera physalus") %>%
  filter(reads > 0)
	
taxon_table %>%
  filter(taxon == "Pusa hispida") %>%
  filter(reads > 0)

taxon_table %>%
  filter(taxon == "Ulcina olrikii") %>%
  filter(reads > 0)

taxon_table %>%
  filter(taxon == "Hexagrammos decagrammus") %>%
  filter(reads > 0)

taxon_table %>%
  filter(taxon == "Salvelinus") %>%
  filter(reads > 0)
```

okay, so using occupancy modeling for pcr reps does not seem the way to go because so many taxon have sporatic detections. 


alright. will need to decide how to decontaminate.  at the moment how similar are sample replicates? 
```{r}
metadata <- read.csv("/home/kimberly.ledger/BeringSea_Arctic_eDNA/data/NBS_SBS_DBO_metadata.csv")
```

```{r}
taxon_table2 <- taxon_table %>%
  separate(sample_ID, into = c("extraction_ID", "replicate"), remove = F) %>%
  filter(sample_type != "positive") %>%
  filter(sample_type != "pcr_blank") %>%
  left_join(metadata)
```

#something from NBS21 
```{r}
taxon_table2 %>%
  filter(sample_type != "field_blank") %>%
  filter(project_year == "NBS_2021") %>%
  filter(location1 == "11") %>%
  ggplot(aes(x=sample_ID, y=reads, fill=taxon)) +
  geom_bar(stat = "identity") + 
  facet_wrap(~extraction_ID, scales = 'free', ncol = 3) +
  theme_bw() + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.position = "none",
    legend.title = element_blank()
  )  

taxon_table2 %>%
  filter(sample_type != "field_blank") %>%
  filter(project_year == "NBS_2021") %>%
  filter(location1 == "11") %>%
  group_by(sample_ID) %>%
  mutate(sum=sum(reads)) %>%
  mutate(prop = reads/sum) %>%
  ggplot(aes(x=sample_ID, y=prop, fill=taxon)) +
  geom_bar(stat = "identity") + 
  facet_wrap(~extraction_ID, scales = 'free', ncol = 3) +
  theme_bw() + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.position = "none",
    legend.title = element_blank()
  )  
```


#something from NBS21 
```{r}
taxon_table2 %>%
  filter(sample_type != "field_blank") %>%
  filter(project_year == "NBS_2021") %>%
  filter(location1 == "8") %>%
  ggplot(aes(x=sample_ID, y=reads, fill=taxon)) +
  geom_bar(stat = "identity") + 
  facet_wrap(~extraction_ID, scales = 'free', ncol = 3) +
  theme_bw() + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.position = "none",
    legend.title = element_blank()
  )  

taxon_table2 %>%
  filter(sample_type != "field_blank") %>%
  filter(project_year == "NBS_2021") %>%
  filter(location1 == "8") %>%
  group_by(sample_ID) %>%
  mutate(sum=sum(reads)) %>%
  mutate(prop = reads/sum) %>%
  ggplot(aes(x=sample_ID, y=prop, fill=taxon)) +
  geom_bar(stat = "identity") + 
  facet_wrap(~extraction_ID, scales = 'free', ncol = 3) +
  theme_bw() + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.position = "none",
    legend.title = element_blank()
  )  
```

higher read counts = more similar compositions across replicates


#something from NBS21 
```{r}
taxon_table2 %>%
  filter(sample_type != "field_blank") %>%
  filter(project_year == "NBS_2021") %>%
  filter(location1 == "46") %>%
  ggplot(aes(x=sample_ID, y=reads, fill=taxon)) +
  geom_bar(stat = "identity") + 
  facet_wrap(~extraction_ID, scales = 'free', ncol = 3) +
  theme_bw() + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.position = "none",
    legend.title = element_blank()
  )  

taxon_table2 %>%
  filter(sample_type != "field_blank") %>%
  filter(project_year == "NBS_2021") %>%
  filter(location1 == "46") %>%
  group_by(sample_ID) %>%
  mutate(sum=sum(reads)) %>%
  mutate(prop = reads/sum) %>%
  ggplot(aes(x=sample_ID, y=prop, fill=taxon)) +
  geom_bar(stat = "identity") + 
  facet_wrap(~extraction_ID, scales = 'free', ncol = 3) +
  theme_bw() + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.position = "none",
    legend.title = element_blank()
  )  
```


how many NBS21 replicates have more than 5000 reads? 
```{r}
nbs21 <- taxon_table2 %>%
  filter(project_year == "NBS_2021") %>%
  group_by(sample_ID) %>%
  summarize(reads = sum(reads))
nbs21
```

```{r}
nbs21 <- nbs21 %>%
  mutate(enough_reads = ifelse(reads > 5000, "yes", "no"))
```

tally the number pcr reps for each extraction rep with enough reads 
```{r}
nbs21 %>%
  separate(sample_ID, into = c("extraction_ID", "replicate"), remove = F) %>%
  filter(enough_reads == "yes") %>%
  group_by(extraction_ID) %>%
  add_tally(name = "n_reps") 
```





